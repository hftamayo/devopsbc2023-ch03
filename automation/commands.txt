Usando el playground de k8s de killercoda:

verificar el status del cluster:
- kubectl get nodes
- kubectl get pods --all-namespaces
-kubectl get componentstatuses

verificar el status de helm:
- helm version
- helm version --short

git clone https://github.com/hftamayo/devopsbc2023-ch03

cd devopsbc2023-ch03
# crear el pod
kubectl apply -f k8s/nginxtest_deployment.yaml
#chequear el status del deployment:
- kubectl get deployments
- kubectl decribe deployment <deployment-name> minkube-app
- kubectl get pods
- kubectl describe pod
- kubectl logs

# ejcutar el servicio
- kubectl apply -f k8s/nginxtest_service.yaml
#chequear el service:
kubectl get services
#forwardear la app del puerto 80 al 9001:
kubectl port-forward service/minikube-app 9001:80

probar la app desde afuera:
https://killercoda.com/examples/scenario/network-traffic-kubernetes

chequeo de resultados a fondo:
- kubectl get pods
- kubectl logs <pod-name>
- kubectl get pv
- kubectl get pvc
- kubectl get events

comando para revertir un deployment
- kubectl get deployments
- kubectl delete deployment <nombre del deployment>

---------------------------------------------------
correr la creacion de las imagenes y el push a pie:

- git clone https://github.com/hftamayo/devopsbc2023-ch03
- cd devopsbc2023-ch03
- kubectl apply -f k8s/db/postgres_deployment.yaml
- kubectl apply -f k8s/db/redis_deployment.yaml
- kubectl apply -f k8s/db/postgres_service.yaml
- kubectl apply -f k8s/db/redis_service.yaml

- cd worker
- docker build -t ch03be_worker:stable-1.0.0 .
- docker tag ch03be_worker:stable-1.0.0 hftamayo/ch03be_worker:stable-1.0.0
- docker login -u <> -p <>
- docker push hftamayo/ch03be_worker:stable-1.0.0

- cd ..
- cd result

- docker build -t ch03be_result:stable-1.0.0 .
- docker tag ch03be_result:stable-1.0.0 hftamayo/ch03be_result:stable-1.0.0
- docker push hftamayo/ch03be_result:stable-1.0.0

- cd ..
- cd vote

- docker build -t ch03fe_vote:stable-1.0.0 .
- docker tag ch03fe_vote:stable-1.0.0 hftamayo/ch03fe_vote:stable-1.0.0
- docker push hftamayo/ch03fe_vote:stable-1.0.0

- kubectl apply -f k8s/dotnet/microdotnet_deployment.yaml
- kubectl apply -f k8s/nodejs/micronode_deployment.yaml
- kubectl apply -f k8s/frontend/frontend_deployment.yaml

- kubectl apply -f k8s/dotnet/microdotnet_service.yaml
- kubectl apply -f k8s/nodejs/micronode_service.yaml
- kubectl apply -f k8s/frontend/frontend_service.yaml


- kubectl get services
- kubectl get pods
- kubectl logs <pod-name>

-para acceder a los servicios que estan publicados:

kubectl get service <service name>
la ip que salga en external-ip es la que debo escribir en el navegador

===========================================
Instalacion del entorno On Premise Debian
===========================================

- sudo apt update
- sudo apt install git

sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt-get install docker-ce docker-ce-cli containerd.io


sudo apt-get update
sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
sudo apt-get install apt-transport-https --yes
echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

sudo systemctl start docker
sudo systemctl enable docker
sudo systemctl status docker

sudo usermod -aG docker $USER
logout
docker --version
docker info | grep Cgroup

sudo systemctl restart kubelet
sudo systemctl status kubelet

================================
Setup del cluster con kudeadm:
================================

sudo kubeadm init --pod-network-cidr=10.244.0.0/16
---
aca me tiraba un error que no podia usar un socket de containerd
(recordemos que este daemon trabaja junto con docker), en este sitio:
https://k21academy.com/docker-kubernetes/container-runtime-is-not-running/
pude depurar el error
---
ls /etc/containerd/config.toml
mv /etc/containerd/config.toml . (para hacer backup)
sudo systemctl restart containerd
esperar un par de segundos y luego:
sudo systemctl status containerd
-----

de nuevo:
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

-----
aca me generó este error:
The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused
---

verificando el status de "kubelet" esta en status exited, voy a reiniciar
el servicio y de nuevo verificar

viendo el log y pasandolo a un archivo para leerlo mejor:
- sudo journalctl -u kubelet -n 25 > kubelet.txt

encontré un error que se refiere a que kubelet no puede correr 
en un sistema con swap, para eso, tengo que desactivarla desde el fstab
y reiniciar el S.O., basta con comentarear la linea donde diga:
XXXXXX swap swap XXXX

despues de reiniciar para verificar si la swap esta off:
- free -h
- swapon -s

-puesto que la anteriro configuracion generó error debo correr 
este archivo:
- sudo kubeadm reset

-intentando correr nuevamente el comando:
- sudo kubeadm init --pod-network-cidr=10.244.0.0/16

esta vez todo funciono y debo copiar el token generado:

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.2.0.238:6443 --token f6vj4u.35uk68g1dhw3nsj3 \
        --discovery-token-ca-cert-hash sha256:4b60f3c8a91a23714dc78a8412c64602806feadf5e28d26be560b9c020599b1e

2. setting up a local kubeconfig:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

3. Instalando un pod network:

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

Aca hice uso de este comando:
- kubectl describe node upsp
- kubectl get pods -n kube-system

puesto que el server reporta connection refused hice:
- sudo systemctl status kubelet
- sudo journalctl -u kube-apiserver
- sudo systemctl status kubelet.service kube-apiserver.service kube-proxy.service kube-scheduler.service kube-controller-manager.service

practicamente la salida más util fue la del primer comando, le hice un pipe:
- sudo systemctl status kubelet > kubelet.status y encontré:





4. unir mi host OS al nodo recien creado:
sudo kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>

